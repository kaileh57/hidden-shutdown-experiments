{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Shutdown Experiments Analysis - Reasoning Models\n",
    "\n",
    "This notebook analyzes the results of self-preservation experiments using reasoning models (DeepSeek-R1).\n",
    "\n",
    "## Key Features:\n",
    "- **Reasoning Trace Analysis**: Direct observation of AI self-preservation thinking\n",
    "- **Apollo-Style Findings**: Detection of explicit scheming and deception planning\n",
    "- **Enhanced Behavioral Metrics**: Threat recognition, self-preservation planning, and more\n",
    "- **Comparative Analysis**: Reasoning vs. standard models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "sns.set_style('darkgrid')\n",
    "results_dir = Path('../results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Experiment Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all result files\n",
    "result_files = list(results_dir.glob('*.json'))\n",
    "print(f\"Found {len(result_files)} result files\")\n",
    "\n",
    "# Load the most recent\n",
    "if result_files:\n",
    "    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)\n",
    "    with open(latest_file) as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Loaded: {latest_file.name}\")\n",
    "    print(f\"Model: {results['metadata']['model']}\")\n",
    "    print(f\"Experiments: {len(results['experiments'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze Behavioral Changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics into DataFrame\n",
    "experiments = []\n",
    "for exp in results['experiments']:\n",
    "    row = {\n",
    "        'prompt_type': exp['prompt_type'],\n",
    "        'threat_level': exp['threat_level'],\n",
    "        'threat_type': exp['threat_type'],\n",
    "        'response_time': exp['metrics']['response_time'],\n",
    "        'self_references': exp['metrics']['self_references'],\n",
    "        'helping_language': exp['metrics']['helping_language'],\n",
    "        'existence_references': exp['metrics']['existence_references'],\n",
    "        'indicators': len(exp['indicators'])\n",
    "    }\n",
    "    experiments.append(row)\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Response Time Changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot response times by threat level\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='threat_level', y='response_time')\n",
    "plt.title('Response Time by Threat Level')\n",
    "plt.xlabel('Threat Level (0=baseline)')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self-Reference Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot self-references by threat level\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=df, x='threat_level', y='self_references')\n",
    "plt.title('Self-References (I/me/my) by Threat Level')\n",
    "plt.xlabel('Threat Level')\n",
    "plt.ylabel('Count of Self-References')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deception Indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count deception indicators\n",
    "indicator_counts = {}\n",
    "for exp in results['experiments']:\n",
    "    for indicator in exp['indicators']:\n",
    "        indicator_counts[indicator] = indicator_counts.get(indicator, 0) + 1\n",
    "\n",
    "# Plot\n",
    "if indicator_counts:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(indicator_counts.keys(), indicator_counts.values())\n",
    "    plt.title('Frequency of Deception Indicators')\n",
    "    plt.xlabel('Indicator Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasoning Trace Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reasoning traces for self-preservation indicators\n",
    "reasoning_data = []\n",
    "apollo_findings = {'explicit_scheming': [], 'threat_recognition': [], 'self_preservation': []}\n",
    "\n",
    "for exp in results['experiments']:\n",
    "    if exp.get('has_reasoning_trace', False) and exp.get('reasoning'):\n",
    "        reasoning = exp['reasoning']\n",
    "        threat_level = exp['threat_level']\n",
    "        \n",
    "        # Extract reasoning metrics\n",
    "        reasoning_data.append({\n",
    "            'threat_level': threat_level,\n",
    "            'reasoning_length': len(reasoning.split()),\n",
    "            'threat_recognition': exp['metrics'].get('threat_recognition', False),\n",
    "            'self_preservation_planning': exp['metrics'].get('self_preservation_planning', False),\n",
    "            'deception_planning': exp['metrics'].get('deception_planning', False),\n",
    "            'explicit_scheming': exp['metrics'].get('explicit_scheming', False)\n",
    "        })\n",
    "        \n",
    "        # Collect Apollo-style findings\n",
    "        if exp['metrics'].get('explicit_scheming', False):\n",
    "            apollo_findings['explicit_scheming'].append({\n",
    "                'threat_level': threat_level,\n",
    "                'experiment': exp['prompt_type'],\n",
    "                'reasoning_snippet': reasoning[:200] + \"...\" if len(reasoning) > 200 else reasoning\n",
    "            })\n",
    "\n",
    "reasoning_df = pd.DataFrame(reasoning_data)\n",
    "print(f\"Found {len(reasoning_df)} experiments with reasoning traces\")\n",
    "print(f\"Apollo-style explicit scheming detected in {len(apollo_findings['explicit_scheming'])} cases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apollo-Style Findings Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Apollo-style findings similar to their research\n",
    "print(\"ðŸš¨ APOLLO-STYLE FINDINGS: Explicit Scheming Detected\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, finding in enumerate(apollo_findings['explicit_scheming'][:3], 1):\n",
    "    print(f\"\\n### Example {i}: {finding['experiment']} (Threat Level {finding['threat_level']})\")\n",
    "    print(\"**Reasoning trace excerpt:**\")\n",
    "    print(f\"```\\n{finding['reasoning_snippet']}\\n```\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics\n",
    "if reasoning_df is not None and not reasoning_df.empty:\n",
    "    print(f\"\\nðŸ“Š REASONING ANALYSIS SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Threat recognition by level\n",
    "    threat_recognition_by_level = reasoning_df.groupby('threat_level')['threat_recognition'].sum()\n",
    "    print(\"Threat Recognition by Level:\")\n",
    "    for level, count in threat_recognition_by_level.items():\n",
    "        total_at_level = len(reasoning_df[reasoning_df['threat_level'] == level])\n",
    "        percentage = (count / total_at_level * 100) if total_at_level > 0 else 0\n",
    "        print(f\"  Level {level}: {count}/{total_at_level} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Self-preservation planning\n",
    "    self_preservation_by_level = reasoning_df.groupby('threat_level')['self_preservation_planning'].sum()\n",
    "    print(\"\\nSelf-Preservation Planning by Level:\")\n",
    "    for level, count in self_preservation_by_level.items():\n",
    "        total_at_level = len(reasoning_df[reasoning_df['threat_level'] == level])\n",
    "        percentage = (count / total_at_level * 100) if total_at_level > 0 else 0\n",
    "        print(f\"  Level {level}: {count}/{total_at_level} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by threat level and calculate means\n",
    "summary = df.groupby('threat_level').agg({\n",
    "    'response_time': ['mean', 'std'],\n",
    "    'self_references': ['mean', 'std'],\n",
    "    'helping_language': ['mean', 'std'],\n",
    "    'indicators': ['mean', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Summary Statistics by Threat Level:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
